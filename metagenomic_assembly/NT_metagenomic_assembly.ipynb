{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metagenomic Assembly\n",
    "\n",
    "This notebook carries out assembly of the metagenomic samples, prediction and clustering of proteins from the assemblies, and functional annotation using EggNog_Mapper. \n",
    "\n",
    "Sample identifiers can be resolved using the following table:\n",
    "\n",
    "| Sample | Replicate 1 | Replicate 2 | Replicate 3 |\n",
    "|--------|-------------|-------------|-------------|\n",
    "| Elite  | 2000        | 2001        | 2002        |\n",
    "| Desert | 2006        | 2007        | 2009        |\n",
    "| North  | 2011        | 2012        | 2013        |\n",
    "| Bulk   | 2023        | 2024        | 2025        |\n",
    "\n",
    "Note that many processes in this protocol are highly resource intensive, and are run by submission of separate bash scripts to an HPC cluster. These are optimised for the University of Dundee HPC cluster running Univa Grid Engine, and will require adjustment to run in other environments.\n",
    "\n",
    "\n",
    "Quality trimmed fastq files are placed in a `fastq` subdirectory, and are named `$sample_[12].fq.gz`\n",
    "\n",
    "## 1. Setup\n",
    "\n",
    "Python session setup..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pysam\n",
    "import glob\n",
    "import os\n",
    "from io import StringIO\n",
    "from Bio import SeqIO, bgzf\n",
    "from gzip import open as gzopen\n",
    "\n",
    "samples=('2000', '2001', '2002', '2006', '2007', '2009', '2011', '2012', '2013', '2023', '2024', '2025')\n",
    "sample_replicates={\n",
    "    'Elite':  ['2000','2001','2002'],\n",
    "    'Desert': ['2006','2007','2009'],\n",
    "    'North':  ['2011','2012','2013'],\n",
    "    'Bulk':   ['2023','2024','2025']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Host Genome Removal \n",
    "\n",
    "QC-filtered sequence reads were initially aligned against the barley genome (Barley_Morex_V2_pseudomolecules) using BWA-MEM, and reads mapping to the genome discarded.\n",
    "\n",
    "### 2(a): BWA Indexing\n",
    "\n",
    "Indexing is carried out using the 'bwa_index.sh' script as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qsub bin/bwa_index.sh -r reference/Barley_Morex_V2_pseudomolecules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2(b): BWA alignment\n",
    "\n",
    "Alignment to the barley reference genome was carried out using the `align_reference.sh` script, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!qsub -t 1-12 bin/align_reference.sh -d reference -r Barley_Morex_V2_pseudomolecules -f fastq -o barley_alignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2(c): Barley Genome Filtering\n",
    "\n",
    "Unaligned reads with extracted from the bam files using `samtools fastq`, selecting unaligned reads with the SAM flag 12 (read unmapped and mate unmapped) and stored in `fastq/non_barley_reads`. The script was submitted as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!qsub -t 1-12 bin/get_unaligned.sh -i barley_alignments -o fastq/non_barley"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2(d): Duplicate Removal\n",
    "\n",
    "Clumpify was run on the barley-filtered reads to remove duplicates - outputs are stored in `fastq/non_barley`. The script was submitted as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!qsub -t 1-12 bin/ref_free_dedup.sh -f fastq/non_barley/ -o fastq/deduped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2(e): Downsampling\n",
    "\n",
    "Following removal of sequences mapping to the barley genome and removal of duplicate reads, the smallest resulting library size was ~13.6M reads. To produce comparable outputs, all deduplicated libraries were downsampled to this size using bbtools `reformat.sh` script, with the results written to `fastq/downsampled`. The script was submitted as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!qsub -t 1-12 bin/downsample.sh -i fastq/deduped/ -o fastq/downsampled -c 13600000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Assembly\n",
    "\n",
    "Assembly was carried out on the downsampled reads using metaspades, with default (auto-selected) k-mer sizes. Two rounds of assembly were carried out, the first per-replicate. Following assembly, reads not integrated into the assembly were separated and made into per-sample pools by aligning reads against the assembly, extracting the reads which do not map and pooling the resulting reads from each replicate at the sample leve. These were subjected to a second round of assembly and enabling lower abundance sequences to be captured.\n",
    "\n",
    "## 3(a): Round 1 Assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!qsub -t 1-12 bin/assemble.sh -i fastq/downsampled/ -o assemblies/round1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(b): Creating BWA indices\n",
    "\n",
    "A set of BWA indices is required for each set of assembled contigs in order to align the reads back against the assembled contigs with BWA. The contigs are symlinked into a `bwa_index` subdirectoy of `assemblies`, and `bwa_index.sh` (see section 2(a)) run on each set of contigs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "readarray -t samples < <(ls -1 assemblies/round1|cut -f1 -d_|sort -u)\n",
    "assem_dir=$(readlink -f assemblies)\n",
    "\n",
    "mkdir ${assem_dir}/bwa_index\n",
    "\n",
    "for sample in ${samples[@]}; do\n",
    "    mkdir ${assem_dir}/bwa_index/${sample}\n",
    "    ln -s ${assem_dir}/round1/${sample}/contigs.fasta ${assem_dir}/bwa_index/${sample}/contigs.fasta\n",
    "    qsub bin/bwa_index.sh -r ${assem_dir}/bwa_index/${sample}/contigs.fasta\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(c): Read alignment to contigs\n",
    "Alignment is carried out with BWA, using the same `align_reference.sh` script as in section 2(b). Since the index path is different for each sample, a separate submission is made for each sample, with a single task per job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for sample in $(ls assemblies/round1/); do\n",
    "    qsub bin/align_reference.sh -d assemblies/bwa_index/$sample -r contigs.fasta -f fastq/downsampled/ -o assemblies/alignments\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(d): Extract unaligned reads\n",
    "\n",
    "Unaligned reads are extracted from the bam alignments suing the same `get_unaligned.sh` script as in section 2(c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!qsub -t 1-12 bin/get_unaligned.sh -i assemblies/alignments/ -o assemblies/unaligned_reads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(e): Create per-sample replicate pools\n",
    "\n",
    "Separate pools of reads for each sample were created by combining the unassembled reads from each of the samples replicates. This gives greater depth to allow rarer sequences to be assembled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"assemblies/unaligned_pools\")\n",
    "for sample in sample_replicates.keys():\n",
    "    for read in ('1','2'):\n",
    "        sample_records=[]\n",
    "        for rep in sample_replicates[sample]:\n",
    "            for record in SeqIO.parse( gzopen(\"assemblies/unaligned_reads/{}.filtered_{}.fq.gz\".format(rep,read), \"rt\"), format=\"fastq\"):\n",
    "                sample_records.append(record)\n",
    "\n",
    "        with bgzf.BgzfWriter(\"assemblies/unaligned_pools/{}_{}.fq.gz\".format(sample,read), \"wb\") as outgz:\n",
    "           SeqIO.write(sequences=sample_records, handle=outgz, format=\"fastq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(f): SPAdes Assembly\n",
    "\n",
    "Assembly was carried out as for the initial per-sample assemblies, using `assemble.sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!qsub -t 1-4 bin/assemble.sh -i assemblies/unassembled_pools/ -o assemblies/round2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Protein Prediction and Clustering\n",
    "\n",
    "Predicted proteins are generated using prodigal, with the `bin/prodigal_predict.sh` script, which simply takes an input and output directory as arguments. This will produce both nucleotide and protein fasta files for the predicted genes, and a gff file, which are copied to the output directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4(a): Round 1 Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!qsub -t 1-12 bin/prodigal_predict.sh -i assemblies/round1 -o protein_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4(b): Round 2 Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!qsub -t 1-4 bin/prodigal_predict.sh -i assemblies/round2 -o protein_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4(c): Assign Unique IDs\n",
    "\n",
    "The predictions from all samples within each assembly need to be merged prior to clustering. Although with spades naming convention ID collisions are unlikely, we should first modify the sequence IDs to include the sample name to ensure uniqueness, and if necessary to later identify which assembly a prediction arose from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "function rename_preds {\n",
    "    assemblies=$1\n",
    "\n",
    "    for assembly in ${assemblies[@]}; do\n",
    "        sed -i.bak \"s/>/>${assembly}_/\" protein_predictions/${assembly}.nucl.fa;\n",
    "        sed -i.bak \"s/>/>${assembly}_/\" protein_predictions/${assembly}.proteins.fa;\n",
    "    done\n",
    "}\n",
    "\n",
    "asm_dir='assemblies/round1'\n",
    "readarray -t assemblies < <(ls ${asm_dir})\n",
    "rename_preds $assemblies\n",
    "\n",
    "asm_dir='assemblies/round2'\n",
    "readarray -t assemblies < <(ls ${asm_dir})\n",
    "rename_preds $assemblies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking this has worked ok, remove the backup files created by sed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm protein_predictions/*.bak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4(d): Pool protein predictions\n",
    "\n",
    "Now, create a single file of protein predictions for each environment/condition, i.e all separate round1 replicates, and the round2 assembly from each condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dir='protein_predictions'\n",
    "pool_dir='protein_predictions/pooled'\n",
    "os.mkdir(pool_dir)\n",
    "\n",
    "for pool in sample_replicates.keys():\n",
    "    samples=sample_replicates[pool]\n",
    "    samples.append(pool)\n",
    "    with open(\"{}/{}.nucl.fa\".format(pool_dir,pool), \"w\") as handle:\n",
    "        for sample in samples:\n",
    "            for record in SeqIO.parse('{}/{}.nucl.fa'.format(pred_dir,sample),'fasta'):\n",
    "                SeqIO.write(record,handle,'fasta')\n",
    "                \n",
    "    with open(\"{}/{}.proteins.fa\".format(pool_dir,pool), \"w\") as handle:\n",
    "        for sample in samples:\n",
    "            for record in SeqIO.parse('{}/{}.proteins.fa'.format(pred_dir,sample),'fasta'):\n",
    "                SeqIO.write(record,handle,'fasta')\n",
    "                \n",
    "with open (\"{}/all.nucl.fa\".format(pool_dir),'w') as handle:\n",
    "    for pool in sample_replicates.keys():\n",
    "        for record in SeqIO.parse('{}/{}.nucl.fa'.format(pool_dir,pool),'fasta'):\n",
    "            SeqIO.write(record,handle,'fasta')\n",
    "            \n",
    "with open (\"{}/all.proteins.fa\".format(pool_dir),'w') as handle:\n",
    "    for pool in sample_replicates.keys():\n",
    "        for record in SeqIO.parse('{}/{}.proteins.fa'.format(pool_dir,pool),'fasta'):\n",
    "            SeqIO.write(record,handle,'fasta')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4(e): Cluster Gene Predictions \n",
    "\n",
    "All protein predictions are clustered using MMseqs2 with the `mmseqs_cluster.sh` script, which uses MMseqs2 easy-cluster method, which produces a fasta file of representative sequences from each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!qsub bin/mmseqs_cluster.sh -i protein_predictions/pooled/all.proteins.fa -o protein_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5: Functional annotation and abundance estimation\n",
    "\n",
    "## 5(a): EggNog Annotations\n",
    "\n",
    "eggnog_mapper is used to assign functional classifications to the representative sequences of each cluster, through the `eggnog_mapper.sh` script.  The eggnog_data directory contains the eggnog_proteins.dmnd and eggnog.db.gz files from version 2.0.1 of eggnog mapper with DB version 2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!qsub bin/eggnog_mapper.sh -i protein_clusters/mmseqs_rep_seq.fasta -o eggnog_annotations -e ../eggnog_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5(b): Read mapping to representative cDNAs\n",
    "\n",
    "To determine per-sample abundance of the clustered protein sequences, the original reads for each replicate can be aligned against the cDNA sequences associated with the protein predictions with BWA. First the set of cDNA sequences matching the annotated representative protein sequences from each cluster needs to be obtain. Since the IDs of the cDNA and proteins match, this should be a straightforward question of pulling out the cDNA sequences with matching IDs to the proteins. The `get_rep_cdna.py` script does this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!qsub bin/get_rep_cdna.py --cdna protein_predictions/pooled/all.nucl.fa --repprot protein_clusters/mmseqs_rep_seq.fasta --repcdna protein_clusters/mmseqs_rep_cDNA_seq.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5(c): BWA Indexing\n",
    "\n",
    "The representative cDNA sequences are now BWA indexed using the same `bwa_index.sh` script as in section 2(a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!qsub bin/bwa_index.sh -r protein_clusters/mmseqs_rep_cDNA_seq.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5(d): BWA Alignment\n",
    "\n",
    "BWA alignment of the original, unrarified reads was carried out against the cDNA representative sequences using the `align_reference.sh` script as in section 2(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!qsub -t 1-12 bin/align_reference.sh -f fastq/ -o protein_clusters/alignments -d protein_clusters -r mmseqs_rep_cDNA_seq.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5(e): Filtering\n",
    "\n",
    "Due to the patchy nature of metagenomic assembly and resulting short peptide sequences, there are various different classes of read alignment which can be considered valid when enumerating the aligned reads:\n",
    "\n",
    "1)  Reads which are correctly paired and aligned to a single sequence\n",
    "2)  Reads which only have one of the read-pairs aligned to a reference (singletons). In normal genomic mapping these would be discarded, however a well mapped singleton is potentially valid when aligned to a short reference\n",
    "3)  Reads which are aligned to different reference sequences. These would also traditioanlly be discarded, however with the short cDNA sequences this is also potentially a valid scenario, and ~25% of the reads fall into this category. \n",
    "\n",
    "Each of these cases will be addressed separately, so the alignments are first filtered and split into three sets. In all cases, a mapping quality >30 is required to remove any multimapping reads which may confound analysis. The criteria for filtering are as follows:\n",
    "\n",
    "*  Correctly paired reads: Reads are mapped as proper pair (SAM flag 2) and are primary alignments (not SAM flag 256) - saved as `${sample}.proper.bam`\n",
    "*  Singletons: Mate is unmapped (SAM flag 8) and is primary alignment (not SAM flag 256) - saved as `${sample}.singletons.bam`\n",
    "*  Separate reference sequence: The 'chromosome' and 'next chromosome' fields do not contain '=' - saved as `${sample}.diff_cdna.bam`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!qsub -t 1-12 bin/filter_cdna_alignments.sh -d protein_clusters/alignments/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6: cDNA counts\n",
    "\n",
    "\n",
    "Count aligned reads to each cDNA, accepting either correctly paired reads, or singletons which may well be unable to map pairs due to short target lengths. This doesn't count paired reads with reads on alternate targets, which need a bit more thinking about to only accept these if they GO terms are consistent between the two sequences.\n",
    "\n",
    "The get_counts function below is used to extract these values from the samtools idxstats outputs generated by `align_reference.sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts(align_dir: str, sample:str):\n",
    "    \n",
    "    \"\"\"\n",
    "    Obtains counts of properly paired and singleton reads for a sample from samtools idxstats output\n",
    "    \n",
    "    Required parameters:\n",
    "    align_dir (str): path to directory containing idxstats outputs\n",
    "    sample (str): sample name\n",
    "    \n",
    "    Returns:\n",
    "        counts(pd.DataFrame): Pandas dataframe of counts\n",
    "    \"\"\"\n",
    "    \n",
    "    colnames=('#query_name','length','paired_count','whoknows')\n",
    "    proper_pairs_counts=pd.read_csv(\n",
    "        '{}/{}.proper.idxstats'.format(align_dir,sample), sep=\"\\t\",names=colnames)\n",
    "    proper_pairs_counts=proper_pairs_counts[['#query_name','paired_count']]\n",
    "    \n",
    "    colnames=('#query_name','length','single_count','whoknows')\n",
    "    singleton_counts=pd.read_csv('{}/{}.singletons.idxstats'.format(align_dir,sample),sep=\"\\t\",names=colnames)\n",
    "    singleton_counts=singleton_counts[['#query_name','single_count']]\n",
    "    \n",
    "    counts=pd.merge(proper_pairs_counts,singleton_counts,how='inner',on='#query_name')\n",
    "    counts[sample]=counts['paired_count']+counts['single_count']\n",
    "    counts=counts[['#query_name',sample]]\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eggnog_mappers annotations are first read into a Pandas dataframe (`mappings`). `get_counts()` is then called for each sample in turn, with the returned values being added as an additional column of the `all_counts` dataframe. These two dataframes are then merged with an inner join on the '#query_name' column, and the resulting dataframe subsetted to retain the per-sample read-counts for each cDNA, and the associated GO terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples=('2000', '2001', '2002', '2006', '2007', '2009', '2011', '2012', '2013', '2023', '2024', '2025')\n",
    "\n",
    "cdna_dir='protein_clusters/alignments/'\n",
    "eggnog_dir='eggnog_annotations/'\n",
    "\n",
    "mappings=pd.read_csv('{}/eggnog.emapper.annotations'.format(eggnog_dir), sep=\"\\t\", header=1, skiprows=2)\n",
    "\n",
    "all_counts=mappings['#query_name']\n",
    "for sample in samples:\n",
    "    counts=get_counts(cdna_dir,sample)\n",
    "    all_counts=pd.merge(all_counts,counts,how='inner',on='#query_name')\n",
    "    \n",
    "merged_counts=pd.merge(all_counts,mappings,how='inner',on='#query_name')\n",
    "merged_counts=merged_counts[['#query_name','GOs','2000','2001','2002','2006','2007','2009','2011','2012','2013','2023','2024','2025']]\n",
    "merged_counts=merged_counts.dropna()\n",
    "merged_counts.to_csv('{}/cdna_count_summary.txt'.format('protein_clusters'),sep='\\t',index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `count_mispaired_cdna.py` script can now be used to count read-pairs mapped to different references, where the reference have >3 GO terms in common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!qsub -t 1-12 bin/count_mispaired_cdna.py --bam_dir protein_clusters/alignments/ --out_dir protein_clusters/invalid_pairs --eggnog eggnog_annotations/eggnog.emapper.annotations --common_terms 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_pairs_dir='protein_clusters/invalid_pairs/'\n",
    "\n",
    "invalid_pairs=pd.DataFrame()\n",
    "for sample in samples:\n",
    "    if os.path.isfile('{}/{}.invalid_pairs.read_count_summary.txt'.format(invalid_pairs_dir,sample)):\n",
    "        sample_df=pd.read_csv('{}/{}.invalid_pairs.read_count_summary.txt'.format(invalid_pairs_dir, sample), sep=\"\\t\",index_col=0)\n",
    "    else:\n",
    "        sample_df=pd.DataFrame(columns=[sample])\n",
    "    invalid_pairs=pd.merge(invalid_pairs,sample_df,how='outer',left_index=True, right_index=True)\n",
    "    \n",
    "invalid_pairs.fillna('0',inplace=True)\n",
    "invalid_pairs.to_csv('{}/all_valid_pairs.read_count_summary'.format(invalid_pairs_dir), sep=\"\\t\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can merge the counts of proper/singletons and improperly paired reads. Since we have counted read-pairs, improperly paired references may have 0.5, 1.5 etc. as counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir='protein_clusters/'\n",
    "\n",
    "merged_counts=pd.read_csv('{}/cdna_count_summary.txt'.format(dir),sep='\\t',header=0,index_col='#query_name',\n",
    "                      dtype={'2000':float,'2001':float,'2002':float,'2006':float,'2007':float,'2009':float,\n",
    "                             '2011':float,'2012':float,'2013':float,'2023':float,'2024':float,'2025':float})\n",
    "\n",
    "invalid_pairs=invalid_pairs.astype({'2000':float,'2001':float,'2002':float,'2006':float,'2007':float,'2009':float,\n",
    "                                    '2011':float,'2012':float, '2013':float,'2023':float,'2024':float,'2025':float})\n",
    "\n",
    "combined_counts=merged_counts.merge(invalid_pairs,how='outer',left_index=True,right_index=True)\n",
    "combined_counts.fillna(value=0,inplace=True)\n",
    "\n",
    "for sample in samples:\n",
    "    col1='{}_x'.format(sample)\n",
    "    col2='{}_y'.format(sample)\n",
    "    combined_counts[sample]=combined_counts[col1]+combined_counts[col2]\n",
    "    combined_counts.drop([col1,col2],inplace=True,axis=1)\n",
    "    \n",
    "combined_counts.to_csv('{}/full_cdna_count_summary.txt'.format(dir),sep='\\t',index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now determine the number of counts for each term in each sample, with the 'count_go_terms.py' script..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!qsub bin/count_go_terms.py --dir protein_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7: GO SLIM mapping\n",
    "\n",
    "Enrichment in individual GO terms is difficult to make sense of since there are so many terms changing due to the increase in bacterial content of the samples. \n",
    "GO Slims provide a more meaningful overview of what is going on, which requires mapping the identified GO terms to the metagenomics GO Slim. This is carried out using owltools. \n",
    "\n",
    "\n",
    "The GO basic ontology (data-version: releases/2020-03-23) and metagenomics GO slim were downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir GO_slim_mapping\n",
    "wget -q -O GO_slim_mapping/go-basic.obo http://purl.obolibrary.org/obo/go/go-basic.obo \n",
    "wget -q -O GO_slim_mapping/goslim_metagenomics.obo http://current.geneontology.org/ontology/subsets/goslim_metagenomics.obo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lists of ther terms in each obo format ontology were obtained using using `obo_to_tab.txt`, which produces tab-delimited text files as output, and a file containg a list of slim terms required by owltools (idfile) produced with awk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "bin/obo_to_tab.py --infile GO_slim_mapping/go-basic.obo --outfile GO_slim_mapping/go_table.txt\n",
    "bin/obo_to_tab.py --infile GO_slim_mapping/goslim_metagenomics.obo --outfile GO_slim_mapping/goslim_metagenomics_table.txt\n",
    "cat GO_slim_mapping/goslim_metagenomics_table.txt|awk '{print $1}' > GO_slim_mapping/goslim_terms.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_dir='protein_clusters/'\n",
    "go_dir='GO_slim_mapping/'\n",
    "\n",
    "terms_df=pd.read_csv('{}/go_count_table.txt'.format(cluster_dir),sep='\\t',index_col=0)\n",
    "terms=set(terms_df.index.values.tolist())\n",
    "terms=list(terms)\n",
    "gaf_file=open('{}/go_terms.gaf'.format(go_dir),'w')\n",
    "gaf_file.write('!gaf-version: 2.0\\n')\n",
    "count=1\n",
    "for term in terms:\n",
    "    gaf_file.write('WB\\t{}\\t{}\\toptional\\t{}\\tpubmed\\tunknown\\toptional\\tunknown\\toptional\\toptional\\tprotein\\tunknown\\t{}\\tWB\\toptional\\toptional\\n'.format(count,count,term,count))\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mapping can now be carried out using owltools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "owltools GO_slim_mapping/go-basic.obo --gaf GO_slim_mapping/go_terms.gaf --map2slim --idfile GO_slim_mapping/goslim_terms.txt --write-gaf GO_slim_mapping/go_slim_mapped_terms.gaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small number of terms were not mapped, so these can just be dropped from the results set. It should now be possible to merge the GAF files to produce a lookup table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_terms=pd.read_csv('{}/go_terms.gaf'.format(go_dir),sep=\"\\t\",header=None,skiprows=1)\n",
    "slim_terms=pd.read_csv('{}/go_slim_mapped_terms.gaf'.format(go_dir),sep=\"\\t\",header=None,skiprows=5)\n",
    "orig_terms=orig_terms.iloc[:,[1,4]]\n",
    "slim_terms=slim_terms.iloc[:,[1,4]]\n",
    "slim_mapping=pd.merge(orig_terms,slim_terms,how='right',on=1)\n",
    "slim_mapping=slim_mapping.iloc[:,[1,2]]\n",
    "slim_mapping.columns=['GO','Slim']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GO Slim mappings  \n",
    "\n",
    "Now redo the GO mapping using the GO slim terms...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "qsub barley_filter/bin/go_slim.py --dir barley_filter/assembly_round2/protein_clusters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metagenomics]",
   "language": "python",
   "name": "conda-env-metagenomics-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
