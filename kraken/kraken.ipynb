{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kraken analysis\n",
    "\n",
    "Standard kraken databases are based upon refseq, which has only around 300 fungal members, and is missing key cereal pathogens, so we need to use a custom database derived from ensembl data, which includes >1000 fungi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from os.path import basename\n",
    "import os\n",
    "import sys\n",
    "from Bio import SeqIO\n",
    "\n",
    "kraken_dir='kraken_db'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to build the database..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Create Ensembl-based kraken database\n",
    "\n",
    "The `bin/dl_kraken_db.py` script downloads all bacteria and fungi sequences from ensembl, in a structure appropriate for building a kraken db. First we need to establish the base structure populated with taxonomy data using kraken-build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "bin/dl_kraken_db.py --db kraken_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "DB_NAME=\"kraken_db\"\n",
    "kraken2-build --download-taxonomy --db $DB_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# iterate through directories to avoid exceeding argument length limits\n",
    "mkdir -p logs\n",
    "for dir in $(ls kraken_db/ensembl); do\n",
    "    ls kraken_db/ensembl/$dir/*.fa|xargs -I{} -n1 -P16 kraken2-build --add-to-library {} --db kraken_db 2>> logs/$dir.build.log\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding barley genome sequences to the database as well..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genome='barley.fasta'\n",
    "out_handle=open('kraken_db/barley/{}'.format(basename(genome)),'w')\n",
    "with open(genome,'rt') as in_handle:\n",
    "    for record in SeqIO.parse(in_handle, 'fasta'):\n",
    "        record.id='{}|kraken:taxid|4513'.format(record.id)\n",
    "        SeqIO.write(record,out_handle,'fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "kraken2-build --add-to-library kraken_db/barley/barley.fasta --db kraken_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "kraken2-build --build -t 16 --db kraken_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Database check\n",
    "\n",
    "Let's get a dump of the database members  using `kraken2-inspect` to ensure what we expect to see is in there..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "kraken2-inspect --threads 24 --db kraken_db > kraken_db_contents.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is ordered with bacteria first, followed by fungi then archae. The taxid can be used to subset the dataframe into separate taxonomic chunks..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxa=pd.read_csv('kraken_db_contents.txt',sep='\\t',header=None)\n",
    "\n",
    "fungi_row=taxa.index[taxa[4]==4751].tolist()[0]\n",
    "archae_row=taxa.index[taxa[4]==2157].tolist()[0]\n",
    "bacteria=taxa.iloc[0:fungi_row-1]\n",
    "fungi=taxa.iloc[fungi_row:archae_row-1]\n",
    "archae=taxa.iloc[archae_row:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can extract the species from each of these sets - note this is just the species, not sub-species etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bacteria_species=bacteria[bacteria[3]=='S']\n",
    "fungi_species=fungi[fungi[3]=='S']\n",
    "archae_species=archae[archae[3]=='S']\n",
    "\n",
    "bacteria_species=bacteria_species[[4,5]]\n",
    "fungi_species=fungi_species[[4,5]]\n",
    "archae_species=archae_species[[4,5]]\n",
    "\n",
    "bacteria_species.columns=('TaxID','Species')\n",
    "fungi_species.columns=('TaxID','Species')\n",
    "archae_species.columns=('TaxID','Species')\n",
    "\n",
    "bacteria_species['Species']=bacteria_species['Species'].apply(lambda x:x.strip())\n",
    "fungi_species['Species']=fungi_species['Species'].apply(lambda x:x.strip())\n",
    "archae_species['Species']=archae_species['Species'].apply(lambda x:x.strip())\n",
    "\n",
    "bacteria_species.to_csv('kraken_bacteria_species.txt',sep='\\t',index=False)\n",
    "fungi_species.to_csv('kraken_fungi_species.txt',sep='\\t',index=False)\n",
    "archae_species.to_csv('kraken_archae_species.txt',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Run kraken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "qsub -t 1-12 bin/kraken.sh -i fastq -o kraken_outputs -d kraken_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output parsing\n",
    "\n",
    "Kraken's output format now needs to be converted to give us read-counts per taxa, including superkindom to allow separation of results.\n",
    "\n",
    "The following functions are used for this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_taxonomy(data_dir):\n",
    "\n",
    "    \"\"\"\n",
    "    Parse the NCBI taxonomy nodes.dmp and names.dmp files to produce a dict containing\n",
    "    the name, rank and parent node id of each node in the database\n",
    "\n",
    "    arguments:\n",
    "    data_dir -- path to directory containing taxonomy files\n",
    "\n",
    "    returns:\n",
    "    nodes -- dict of dicts\n",
    "    \"\"\"\n",
    "\n",
    "    # The nodes.dmp file is delimited by '\\t|\\t' and contains the following\n",
    "    # fields (from https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdump_readme.txt)\n",
    "\n",
    "    #tax_id                 -- the id of node associated with this name\n",
    "    #name_txt               -- name itself\n",
    "    #unique name                -- the unique variant of this name if name not unique\n",
    "    #name class             -- (synonym, common name, ...)\n",
    "\n",
    "    # The species names are found in the 'scientific name' lines\n",
    "\n",
    "    names = dict()\n",
    "    nodes = dict()\n",
    "\n",
    "    for line in open(data_dir + '/names.dmp','r'):\n",
    "        fields = line.rstrip(\"\\n\").split(\"\\t|\\t\")\n",
    "        if 'scientific name' in fields[3]:\n",
    "            names[int(fields[0])] = fields[1]\n",
    "    \n",
    "    # The nodes.dmp file is delimited by '\\t|\\t' and contains the following\n",
    "    # fields (from https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdump_readme.txt)\n",
    "\n",
    "    # tax_id -- node id in GenBank taxonomy database\n",
    "    # parent tax_id             -- parent node id in GenBank taxonomy database\n",
    "    # rank                  -- rank of this node (superkingdom, kingdom, ...)\n",
    "    # embl code             -- locus-name prefix; not unique\n",
    "    # division id               -- see division.dmp file\n",
    "    # ....\n",
    "\n",
    "    for line in open(data_dir + '/nodes.dmp', 'r'):\n",
    "        fields = line.rstrip(\"\\n\").split(\"\\t|\\t\")\n",
    "        node_data = {\n",
    "            'parent': int(fields[1]),\n",
    "            'rank': fields[2],\n",
    "            'name': names[int(fields[0])]\n",
    "        }\n",
    "        nodes[int(fields[0])] = node_data\n",
    "    \n",
    "    f=lambda x: x.replace('\\t|','')\n",
    "    tax_names=pd.read_csv('{}/names.dmp'.format(data_dir), sep=\"\\t\\|\\t\",header=0,engine='python',converters={3:f},\n",
    "                      names=['taxid','name','dontknow','type'])\n",
    "    tax_names=tax_names.loc[tax_names['type']=='scientific name']\n",
    "    tax_names=tax_names[['taxid','name']]\n",
    "\n",
    "    return(nodes,tax_names)\n",
    "\n",
    "def walk_tree(tax_id):\n",
    "\n",
    "    \"\"\"\n",
    "    Walk up taxonomy tree from provided taxid until superkingdom  node is found, which is returned\n",
    "    This enables us to separate taxa into bacteria, fungi and archae\n",
    "\n",
    "    arguments:\n",
    "        taxid -- string\n",
    "\n",
    "    returns:\n",
    "        superkingdom -- string\n",
    "    \"\"\"\n",
    "    \n",
    "    if int(tax_id) in nodes:\n",
    "        node_data=nodes[int(tax_id)]\n",
    "        rank=node_data['rank']\n",
    "        while node_data['rank'] != 'superkingdom':\n",
    "            node_data=nodes[node_data['parent']]\n",
    "            name=node_data['name']\n",
    "            rank=node_data['rank']\n",
    "            \n",
    "        return(name) \n",
    "\n",
    "def read_file(file):\n",
    "    \n",
    "    \"\"\"\n",
    "    Parses a kraken report file (see https://ccb.jhu.edu/software/kraken/MANUAL.html).\n",
    "    This is a tab-delimited file containing the following fields:\n",
    "    \n",
    "    * Percentage of reads covered by the clade rooted at this taxon\n",
    "    * Number of reads covered by the clade rooted at this taxon\n",
    "    * Number of reads assigned directly to this taxon\n",
    "    * A rank code, indicating (U)nclassified, (D)omain, (K)ingdom, (P)hylum, (C)lass, (O)rder, (F)amily, (G)enus, or (S)pecies. \n",
    "    * NCBI taxonomy ID\n",
    "    * indented scientific name\n",
    "\n",
    "    This function reads the file, and selects just the (D)omain, (P)hylym, (C)lass,\n",
    "    (O)rder and (F)amily ranks.\n",
    "    \n",
    "    For each rank of Phylum, Class, Order and Family, the rank, NCBI taxid and read count are \n",
    "    selected. The taxonomy tree is then walked using the taxid to obtain the superkingdom for the taxa, \n",
    "    which is added to the dataframe.\n",
    "    \n",
    "    arguments:\n",
    "        file(str): path of kraken report file to parse\n",
    "        \n",
    "    Returns:\n",
    "        res(dict): Dict of abundance dataframes, keyed on rank\n",
    "    \"\"\"\n",
    "    \n",
    "    file_df=pd.read_csv(file,sep='\\t',header=None)\n",
    "    # Select just domain, phylum, class, order and family...\n",
    "    file_df=file_df[(file_df[3] == 'D') | (file_df[3] == 'P') | (file_df[3] == 'C') \n",
    "                    | (file_df[3] == 'O') | (file_df[3] == 'F')]\n",
    "    \n",
    "    res=dict()\n",
    "    levels=('Phylum','Class','Order','Family')\n",
    "    for level in levels:\n",
    "        l=level[0]\n",
    "    \n",
    "        df=file_df[[5,4,1]][file_df[3]==l]\n",
    "    \n",
    "        df.columns=(level,'Taxid','Count')\n",
    "        df.reset_index(inplace=True)\n",
    "        df=df[[level,'Taxid','Count']]\n",
    "        df[level]=df[level].apply(lambda x:x.strip())\n",
    "        df['Superkingdom']=df['Taxid'].apply(lambda x:walk_tree(x))\n",
    "        res[level]=df\n",
    "    return(res)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, parse the taxonomy files to give us a pandas DataFrame of taxonomy id to name mappings (names), and a dictionary (nodes) containing each node keyed on the taxonomy id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='{}/taxonomy'.format(kraken_dir)\n",
    "nodes,names=parse_taxonomy(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain a list of the output reports generated by kraken, create a directory for storing the parsed results and create a dictionary mapping sample IDs to names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=glob.glob('kraken_outputs/*.report.txt')\n",
    "\n",
    "if not os.path.exists('parsed_outputs'):\n",
    "    os.mkdir('parsed_outputs')\n",
    "\n",
    "samples={\n",
    "    '2000':'Elite_1',  '2001':'Elite_2',  '2002':'Elite_3',\n",
    "    '2006':'Desert_1', '2007':'Desert_2', '2009':'Desert_3',\n",
    "    '2011':'North_1',  '2012':'North_2',  '2013':'North_3',\n",
    "    '2023':'Bulk_1',   '2024':'Bulk_2',   '2025':'Bulk_3'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over the report files, reading each in turn and writing each of the resulting dataframes into separate text files, split on superkingdom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    fn=basename(file)\n",
    "    sample=samples[fn.split('.')[0]]\n",
    "    results=read_file(file)\n",
    "    for result in results.keys():\n",
    "        df=results[result]\n",
    "        bact_df=df[df['Superkingdom']=='Bacteria']\n",
    "        fun_df=df[df['Superkingdom']=='Eukaryota']\n",
    "        arch_df=df[df['Superkingdom']=='Archaea']\n",
    "        \n",
    "        bact_fn='parsed_outputs/{}_{}_{}.txt'.format(sample,'bacteria',result.lower())\n",
    "        fun_fn='parsed_outputs/{}_{}_{}.txt'.format(sample,'fungi',result.lower())\n",
    "        arch_fn='parsed_outputs/{}_{}_{}.txt'.format(sample,'archaea',result.lower())\n",
    "        \n",
    "        bact_df.to_csv(bact_fn,sep='\\t',index=False)\n",
    "        fun_df.to_csv(fun_fn,sep='\\t',index=False)\n",
    "        arch_df.to_csv(arch_fn,sep='\\t',index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots of the results are produced using the `kraken_plots.Rmd` R markdown document."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kraken_test]",
   "language": "python",
   "name": "conda-env-kraken_test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
